{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.prepare_data import create_dataloaders\n",
    "from dataclasses import dataclass\n",
    "from main.models import ConvNet\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "import laplace\n",
    "import torch.nn.functional as F\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set configurations\n",
    "@dataclass\n",
    "class ActiveLearningConfigMNIST:\n",
    "    subset_of_weights: str = 'last_layer'\n",
    "    hessian_structure: str = 'kron'\n",
    "    backend: str = 'AsdlGGN'\n",
    "    temperature: float = 1\n",
    "    max_training_samples: int = 500\n",
    "    acquisition_batch_size: int = 100\n",
    "    al_method: str = 'random'\n",
    "    test_batch_size: int =512\n",
    "    num_classes: int = 10\n",
    "    num_initial_samples: int = 50\n",
    "    training_iterations: int = 4096 * 6\n",
    "    scoring_batch_size: int = 64\n",
    "    train_batch_size: int = 64\n",
    "    extract_pool: int = 55000  # number of samples to extract from the dataset (bit of a hack)\n",
    "    dataset: str = 'fashion_mnist'\n",
    "\n",
    "config = ActiveLearningConfigMNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\Documents\\Statistics\\TT\\msc_thesis\\.venv\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:64: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.\n",
      "  warnings.warn(\"`data_source` argument is not used and will be removed in 2.2.0.\"\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader, pool_loader, active_loader = create_dataloaders(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\ '\n",
      "C:\\Users\\vince\\AppData\\Local\\Temp\\ipykernel_20788\\2464401586.py:7: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def badge_selection(model, pool_dataset, batch_size):\n",
    "    \"\"\"\n",
    "    Implement the BADGE selection strategy for a single iteration.\n",
    "    \n",
    "    Args:\n",
    "    - model: PyTorch model\n",
    "    - pool_dataset: Dataset containing unlabeled examples (U \\ S)\n",
    "    - batch_size: Number of examples to select (B)\n",
    "    \n",
    "    Returns:\n",
    "    - indices: Indices of selected examples to be added to S\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    gradient_embeddings = []\n",
    "    \n",
    "    # Compute gradient embeddings for all examples in U \\ S\n",
    "    for idx, (x, _) in enumerate(pool_dataset):\n",
    "        x = x.unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(x)\n",
    "        \n",
    "        # Compute hypothetical label\n",
    "        y_hat = output.argmax(dim=1)\n",
    "        \n",
    "        # Compute gradient embedding\n",
    "        loss = F.cross_entropy(output, y_hat)\n",
    "        \n",
    "        # Compute gradients w.r.t. the last layer parameters\n",
    "        grad_embedding = torch.autograd.grad(loss, model.get_last_layer_parameters(), create_graph=False)[0]\n",
    "        \n",
    "        gradient_embeddings.append(grad_embedding.cpu().detach().numpy().flatten())\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    gradient_embeddings = np.array(gradient_embeddings)\n",
    "    \n",
    "    # Use k-MEANS++ to select diverse samples\n",
    "    kmeans = KMeans(n_clusters=batch_size, init='k-means++', n_init=1, max_iter=1)\n",
    "    kmeans.fit(gradient_embeddings)\n",
    "    \n",
    "    # Get the indices closest to the centroids\n",
    "    distances = kmeans.transform(gradient_embeddings)\n",
    "    selected_indices = np.argmin(distances, axis=0)\n",
    "    \n",
    "    return selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 445, 1164, 3554, 3170, 1052,  355, 2336, 4115, 4125,  111, 4165,\n",
       "       2921, 3434, 1353, 2835, 3479,  345, 1899,  450,  674, 4548,  901,\n",
       "        317, 2106, 1664, 3300,  313, 4729, 2574, 2615, 4276, 1351, 4316,\n",
       "       4032, 1674, 3775, 3005, 2518, 2480, 3642, 3839,  737, 2189,  392,\n",
       "       2565, 4663, 1952, 4427, 2582, 3513, 3306, 3378,  508, 4056, 2537,\n",
       "       2426,  853, 4551, 3044, 4499, 1719,  699,  787, 3025, 2787, 2255,\n",
       "       4841, 2358, 4559, 2415, 1120, 3437,   96, 4790, 4795, 2748, 3668,\n",
       "        341, 1942, 3140,  974, 2843, 1194, 3485, 3700, 1565,  977, 3951,\n",
       "       1242, 2383, 2531,  849, 4376, 1434, 3698, 3634, 1327, 2552, 3601,\n",
       "       3474], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "badge_selection(model=ConvNet(), pool_dataset=pool_loader.dataset, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "la = laplace.Laplace(model=ConvNet(),\n",
    "                     likelihood='classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x0000027CD3EFECE0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvNet().get_last_layer_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer = list(la.model.parameters())[-2:]\n",
    "last_layer = (y for y in last_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object <genexpr> at 0x0000027CD3995E40>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
