{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\Documents\\Statistics\\TT\\msc_thesis\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from main.models import ConvNet, get_model_constructor\n",
    "from main.utils import save_experiment\n",
    "from dataclasses import dataclass\n",
    "from main.active_learning import run_active_learning\n",
    "from main.prepare_data import create_dataloaders\n",
    "from configurations import get_config\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# pick 10 super random numbers\n",
    "fixed_seeds = [12345, 23456, 34567, 45678, 56789, 67890, 78901, 89012, 90123, 12340]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running similarity_kmeans_mnist_20_to_200_B=10 experiment\n"
     ]
    }
   ],
   "source": [
    "config, experiment_name = get_config(\n",
    "    method='similarity_kmeans',\n",
    "    min_samples=20,\n",
    "    max_samples=200,\n",
    "    acquisition_batch_size=10,\n",
    "    dataset='mnist',\n",
    "    )\n",
    "\n",
    "config.extract_pool = 59900\n",
    "\n",
    "experiment_name = experiment_name\n",
    "\n",
    "print(f'Running {experiment_name} experiment')\n",
    "save_results = True\n",
    "num_runs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\Documents\\Statistics\\TT\\msc_thesis\\.venv\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:64: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.\n",
      "  warnings.warn(\"`data_source` argument is not used and will be removed in 2.2.0.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 10000 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Set Size:  10%|█         | 20/200 [00:00<?, ?it/s]2024-08-31 12:02:28,402 - INFO - Training set size: 20, Test set accuracy: 61.62, Test set loss: 0.0035\n",
      "c:\\Users\\vince\\Documents\\Statistics\\TT\\msc_thesis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "c:\\Users\\vince\\Documents\\Statistics\\TT\\msc_thesis\\.venv\\Lib\\site-packages\\laplace\\baselaplace.py:409: UserWarning: By default `link_approx` is `probit`. Make sure to set it equals to the way you want to call `la(test_data, pred_type=..., link_approx=...)`.\n",
      "  warnings.warn(\n",
      "Training Set Size:  15%|█▌        | 30/200 [00:17<04:54,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HP DEVICE: cpu\n",
      "Precision DEVICE: cpu\n",
      "torch.float32\n",
      "#Samps\tTotal Distance\n",
      "1\t50.857357025146484\n",
      "2\t12.149543762207031\n",
      "3\t11.793352127075195\n",
      "4\t11.175128936767578\n",
      "5\t10.823549270629883\n",
      "6\t10.351934432983398\n",
      "7\t9.975040435791016\n",
      "8\t9.520711898803711\n",
      "9\t9.201507568359375\n",
      "Dataset indices:  [51303 48691 39082 13779 48193 35465 27389 20952  2517  4084]\n",
      "Scores:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Labels:  tensor([0, 7, 7, 0, 3, 0, 3, 3, 2, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset), \u001b[38;5;28mlen\u001b[39m(test_loader\u001b[38;5;241m.\u001b[39mdataset), \u001b[38;5;28mlen\u001b[39m(pool_loader\u001b[38;5;241m.\u001b[39mdataset))\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# get results\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_active_learning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactive_learning_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactive_learning_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_constructor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_model_constructor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mal_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# save results and configuration\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_results:\n",
      "File \u001b[1;32mc:\\Users\\vince\\Documents\\Statistics\\TT\\msc_thesis\\main\\active_learning.py:70\u001b[0m, in \u001b[0;36mrun_active_learning\u001b[1;34m(train_loader, test_loader, pool_loader, active_learning_data, model_constructor, config, device)\u001b[0m\n\u001b[0;32m     66\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(logits\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     68\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(prediction, target)\n\u001b[1;32m---> 70\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Test\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vince\\Documents\\Statistics\\TT\\msc_thesis\\.venv\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vince\\Documents\\Statistics\\TT\\msc_thesis\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vince\\Documents\\Statistics\\TT\\msc_thesis\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(num_runs):\n",
    "    current_seed = fixed_seeds[i]\n",
    "    torch.manual_seed(current_seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(current_seed)\n",
    "        torch.cuda.manual_seed_all(current_seed)\n",
    "\n",
    "    # load data\n",
    "    train_loader, test_loader, pool_loader, active_learning_data = create_dataloaders(config)\n",
    "\n",
    "    print(len(train_loader.dataset), len(test_loader.dataset), len(pool_loader.dataset))\n",
    "    \n",
    "    # get results\n",
    "    results = run_active_learning(\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader, \n",
    "        pool_loader=pool_loader,\n",
    "        active_learning_data=active_learning_data,\n",
    "        model_constructor=get_model_constructor(config.al_method, config.dataset), \n",
    "        config=config, \n",
    "        device=device\n",
    "        )\n",
    "\n",
    "    # save results and configuration\n",
    "    if save_results:\n",
    "        experiment_id = experiment_name + '_' + str(i + 1)\n",
    "        save_experiment(config, results, experiment_id, base_dir='pseudo_label_experiments')\n",
    "    \n",
    "    del train_loader, test_loader, pool_loader, active_learning_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
