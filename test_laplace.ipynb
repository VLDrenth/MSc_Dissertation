{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import laplace\n",
    "import numpy as np\n",
    "import torch.utils\n",
    "import matplotlib.pyplot as plt\n",
    "from main.models import ConvNet, BayesianConvNet\n",
    "from main.training_models import train_model\n",
    "from main.utils import entropy\n",
    "from batchbald_redux import repeated_mnist, joint_entropy, batchbald\n",
    "from laplace.utils.utils import normal_samples\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mnist data\n",
    "train_dataset, val_dataset = repeated_mnist.create_MNIST_dataset()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32,\n",
    "                                           sampler=torch.utils.data.SubsetRandomSampler(range(1000)))\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, sampler=torch.utils.data.SubsetRandomSampler(range(1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 1.3662784099578857\n",
      "Epoch 2/25, Loss: 1.1333781480789185\n",
      "Epoch 3/25, Loss: 0.5565322041511536\n",
      "Epoch 4/25, Loss: 0.211321160197258\n",
      "Epoch 5/25, Loss: 0.1780632883310318\n",
      "Epoch 6/25, Loss: 0.12224686145782471\n",
      "Epoch 7/25, Loss: 0.10107780992984772\n",
      "Epoch 8/25, Loss: 0.08403026312589645\n",
      "Epoch 9/25, Loss: 0.45377659797668457\n",
      "Epoch 10/25, Loss: 0.04758601635694504\n",
      "Epoch 11/25, Loss: 0.039059266448020935\n",
      "Epoch 12/25, Loss: 0.15554024279117584\n",
      "Epoch 13/25, Loss: 0.006701083853840828\n",
      "Epoch 14/25, Loss: 0.011465385556221008\n",
      "Epoch 15/25, Loss: 0.04479203000664711\n",
      "Epoch 16/25, Loss: 0.0017860461957752705\n",
      "Epoch 17/25, Loss: 0.0054418547078967094\n",
      "Epoch 18/25, Loss: 0.0011644844198599458\n",
      "Epoch 19/25, Loss: 0.0010600830428302288\n",
      "Epoch 20/25, Loss: 0.002040612045675516\n",
      "Epoch 21/25, Loss: 0.006248588673770428\n",
      "Epoch 22/25, Loss: 0.019717438146471977\n",
      "Epoch 23/25, Loss: 0.004651464056223631\n",
      "Epoch 24/25, Loss: 0.0004772499087266624\n",
      "Epoch 25/25, Loss: 0.000592651660554111\n",
      "Accuracy of the network on the validation images: 92 %\n"
     ]
    }
   ],
   "source": [
    "model = train_model(ConvNet(), train_loader, num_epochs=25, lr=1e-3)\n",
    "\n",
    "# evaluate model on validation set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the validation images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse data from training_loader into tensor for x and y\n",
    "x = torch.cat([x for x, y in train_loader], dim=0)\n",
    "y = torch.cat([y for x, y in train_loader], dim=0)\n",
    "\n",
    "x_test = torch.cat([x for x, y in val_loader], dim=0)[:50]\n",
    "y_test = torch.cat([y for x, y in val_loader], dim=0)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the Laplace approximation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Computing Hessian]: 100%|██████████| 32/32 [00:03<00:00, 10.51it/s]\n",
      "c:\\Users\\vince\\Documents\\Statistics\\TT\\msc_thesis\\.venv\\Lib\\site-packages\\laplace\\baselaplace.py:409: UserWarning: By default `link_approx` is `probit`. Make sure to set it equals to the way you want to call `la(test_data, pred_type=..., link_approx=...)`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing the prior precision...\n"
     ]
    }
   ],
   "source": [
    "# User-specified LA flavor\n",
    "la = laplace.Laplace(model,\n",
    "                     likelihood=\"classification\",\n",
    "                     subset_of_weights='all',\n",
    "                     hessian_structure='kron',\n",
    "                     enable_backprop=True\n",
    "                     )\n",
    "\n",
    "print('Fitting the Laplace approximation...')\n",
    "la.fit(train_loader, progress_bar=True)\n",
    "\n",
    "print('Optimizing the prior precision...')\n",
    "la.optimize_prior_precision(\n",
    "    method='marglik'\n",
    ")\n",
    "\n",
    "# User-specified predictive approx.\n",
    "#print('Computing the predictive distribution...')\n",
    "#pred = la(x, pred_type=\"glm\", link_approx=\"probit\")  # probabilities (N x C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = la(x_test, pred_type=\"glm\", link_approx='probit')\n",
    "samples = la.predictive_samples(x_test, n_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch 0 and 1 dimensions\n",
    "p_samples = torch.swapaxes(samples, 0, 1)\n",
    "log_probs = torch.log(p_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean is (N, C) and variance is (N, C, C)\n",
    "f_mu, f_var = la._glm_predictive_distribution(x_test)\n",
    "\n",
    "# obtain samples of predicted network outputs f. f_samples is (n_samples, N, C)\n",
    "# using softmax would give us the probabilities as in la.predictive_samples\n",
    "\n",
    "n_samples = 1000\n",
    "f_samples = normal_samples(f_mu, f_var, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-zero eigenvalues: 500 out of 500\n"
     ]
    }
   ],
   "source": [
    "# turn f_var into block diagonal matrix of size NC x NC\n",
    "N = x_test.shape[0]\n",
    "C = 10\n",
    "\n",
    "f_var_block = torch.zeros(N * C, N * C)\n",
    "for i in range(N):\n",
    "    f_var_block[i*C:(i+1)*C, i*C:(i+1)*C] = f_var[i]\n",
    "\n",
    "# compute eigenvalues of f_var_block\n",
    "eigvals = np.linalg.eigvals(f_var_block.detach())\n",
    "nonzeros = np.sum(eigvals > 1e-6)\n",
    "print(f'Number of non-zero eigenvalues: {(nonzeros)} out of {N*C}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional (co)variance using Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jacobian(x, model, la):\n",
    "    num_params = la.n_params\n",
    "\n",
    "    # Computing the Jacobian of the model with respect to the parameters\n",
    "    J = torch.zeros(x.size(0), 10, num_params)\n",
    "    for i in range(x.size(0)):\n",
    "        model.zero_grad()\n",
    "        output = model(x[i].unsqueeze(0))\n",
    "\n",
    "        for c in range(10):\n",
    "            output[0, c].backward(retain_graph=True)\n",
    "            # select the last parameters of the last layer \n",
    "            last_layer_gradients = torch.cat([p.grad.flatten() for p in model.parameters()])[-num_params:]\n",
    "\n",
    "            J[i, c, :] = last_layer_gradients\n",
    "    return J.detach()\n",
    "\n",
    "J = compute_jacobian(x, model, la)\n",
    "\n",
    "# fraction of zeros in the Jacobian\n",
    "(J == 0).sum() / J.numel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
