{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from batchbald_redux import repeated_mnist, active_learning, batchbald\n",
    "from main.models import BayesianConvNet\n",
    "from main.utils import save_experiment, load_experiment\n",
    "\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:64: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.\n",
      "  warnings.warn(\"`data_source` argument is not used and will be removed in 2.2.0.\"\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "train_dataset, test_dataset = repeated_mnist.create_MNIST_dataset()\n",
    "\n",
    "# number of initial samples \n",
    "num_initial_samples = 40\n",
    "num_classes = 10\n",
    "\n",
    "# get indices of initial samples\n",
    "initial_samples = active_learning.get_balanced_sample_indices(\n",
    "    repeated_mnist.get_targets(train_dataset), num_classes=num_classes, n_per_digit=num_initial_samples / num_classes\n",
    ")\n",
    "\n",
    "# Experiment parameters\n",
    "max_training_samples = 100  # Maximum number of samples to acquire from the pool dataset \n",
    "acquisition_batch_size = 5  # Number of samples to acquire in each acquisition step\n",
    "num_inference_samples = 50  # Number of samples to use for inference in MC-Dropout\n",
    "num_test_inference_samples = 5  \n",
    "num_samples = 100000 # Number of samples to use for estimation in batchbald\n",
    "\n",
    "test_batch_size = 512  # Batch size for testing\n",
    "batch_size = 64  # Batch size for training\n",
    "scoring_batch_size = 128  # Batch size for scoring \n",
    "training_iterations = 4096 * 6 # Number of training iterations (batches) to run\n",
    "\n",
    "kwargs = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = \"cuda\" if use_cuda else \"cpu\"\n",
    "\n",
    "print(f\"use_cuda: {use_cuda}\")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "active_learning_data = active_learning.ActiveLearningData(train_dataset)\n",
    "\n",
    "# Split off the initial samples first.\n",
    "active_learning_data.acquire(initial_samples)\n",
    "\n",
    "# THIS REMOVES MOST OF THE POOL DATA. UNCOMMENT THIS TO TAKE ALL UNLABELLED DATA INTO ACCOUNT!\n",
    "active_learning_data.extract_dataset_from_pool(55000)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    active_learning_data.training_dataset,\n",
    "    sampler=active_learning.RandomFixedLengthSampler(active_learning_data.training_dataset, training_iterations),\n",
    "    batch_size=batch_size,\n",
    "    **kwargs,\n",
    ")\n",
    "\n",
    "pool_loader = torch.utils.data.DataLoader(\n",
    "    active_learning_data.pool_dataset, batch_size=scoring_batch_size, shuffle=False, **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Set Size:  40%|████      | 40/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0021, Accuracy: 7612/10000 (76.12%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Conditional Entropy: 100%|██████████| 4960/4960 [00:00<00:00, 6199.33it/s]\n",
      "Entropy: 100%|██████████| 4960/4960 [00:00<00:00, 7437.85it/s]\n",
      "Training Set Size:  45%|████▌     | 45/100 [02:11<24:06, 26.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset indices:  [14887 25860 31981 19524 33594]\n",
      "Scores:  [1.2570762080746538, 1.2057781946211683, 1.1822914491967982, 1.1785762716961254, 1.165321742461618]\n",
      "Labels:  tensor([5, 6, 6, 2, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0017, Accuracy: 7894/10000 (78.94%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Conditional Entropy: 100%|██████████| 4955/4955 [00:00<00:00, 7455.32it/s]\n",
      "Entropy: 100%|██████████| 4955/4955 [00:00<00:00, 6889.08it/s]\n",
      "Training Set Size:  50%|█████     | 50/100 [04:30<22:39, 27.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset indices:  [  776 54433  2278 24440 10744]\n",
      "Scores:  [1.145820523819269, 1.112182246310169, 1.0939686979812069, 1.0932006801970324, 1.055648387434773]\n",
      "Labels:  tensor([0, 0, 0, 0, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0015, Accuracy: 8044/10000 (80.44%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Conditional Entropy: 100%|██████████| 4950/4950 [00:00<00:00, 7239.24it/s]\n",
      "Entropy: 100%|██████████| 4950/4950 [00:00<00:00, 7130.28it/s]\n",
      "Training Set Size:  55%|█████▌    | 55/100 [06:46<20:25, 27.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset indices:  [37605 12497 22155 13548 11406]\n",
      "Scores:  [1.387942584076684, 1.2406741511173915, 1.2268208119576713, 1.2233152210827716, 1.2101442161094322]\n",
      "Labels:  tensor([8, 0, 3, 4, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0019, Accuracy: 7979/10000 (79.79%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Conditional Entropy: 100%|██████████| 4945/4945 [00:00<00:00, 7042.46it/s]\n",
      "Entropy: 100%|██████████| 4945/4945 [00:00<00:00, 6951.87it/s]\n",
      "Training Set Size:  60%|██████    | 60/100 [09:03<18:11, 27.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset indices:  [52151 31772   328 59670 36892]\n",
      "Scores:  [1.307985834851686, 1.236030597124699, 1.196492569629201, 1.165299091742384, 1.1536458123570186]\n",
      "Labels:  tensor([9, 5, 5, 5, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0016, Accuracy: 8080/10000 (80.80%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Conditional Entropy: 100%|██████████| 4940/4940 [00:00<00:00, 7239.95it/s]\n",
      "Entropy: 100%|██████████| 4940/4940 [00:00<00:00, 6913.91it/s]\n",
      "Training Set Size:  65%|██████▌   | 65/100 [11:27<16:13, 27.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset indices:  [16902 26613 51937  5259 50736]\n",
      "Scores:  [1.2336760161933784, 1.2185946758179074, 1.1857944817189607, 1.1794804010600253, 1.1730633718922934]\n",
      "Labels:  tensor([2, 2, 2, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0014, Accuracy: 8338/10000 (83.38%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Conditional Entropy: 100%|██████████| 4935/4935 [00:00<00:00, 7069.33it/s]\n",
      "Entropy: 100%|██████████| 4935/4935 [00:00<00:00, 6936.38it/s]\n",
      "Training Set Size:  70%|███████   | 70/100 [13:41<13:43, 27.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset indices:  [32473 11241 17269 32467 20191]\n",
      "Scores:  [1.1839491458183513, 1.0621347429943357, 1.0379804434159303, 1.0209799419773917, 1.0117220900624573]\n",
      "Labels:  tensor([8, 8, 2, 8, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0013, Accuracy: 8322/10000 (83.22%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Conditional Entropy: 100%|██████████| 4930/4930 [00:00<00:00, 7077.54it/s]\n",
      "Entropy: 100%|██████████| 4930/4930 [00:00<00:00, 7034.65it/s]\n",
      "Training Set Size:  75%|███████▌  | 75/100 [15:55<11:20, 27.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset indices:  [36984 15424 42148 31222 38511]\n",
      "Scores:  [1.1278223609987927, 1.0701017336639933, 1.066308532998786, 1.061724681440225, 1.0555405330912768]\n",
      "Labels:  tensor([5, 8, 7, 5, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0010, Accuracy: 8683/10000 (86.83%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Conditional Entropy: 100%|██████████| 4925/4925 [00:01<00:00, 3720.61it/s]\n",
      "Entropy: 100%|██████████| 4925/4925 [00:01<00:00, 3846.00it/s]\n",
      "Training Set Size:  80%|████████  | 80/100 [18:12<09:06, 27.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset indices:  [44228 49525 28420 11378  2535]\n",
      "Scores:  [1.0596260216946538, 1.0481973521740566, 1.0221043609324565, 1.0204017743479799, 0.9984595105457427]\n",
      "Labels:  tensor([9, 8, 1, 4, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0011, Accuracy: 8571/10000 (85.71%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Conditional Entropy: 100%|██████████| 4920/4920 [00:00<00:00, 7153.93it/s]\n",
      "Entropy: 100%|██████████| 4920/4920 [00:00<00:00, 7180.05it/s]\n",
      "Training Set Size:  85%|████████▌ | 85/100 [20:28<06:48, 27.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset indices:  [45101 44480  7856 20636  5295]\n",
      "Scores:  [1.167979877812738, 1.1387607188794846, 1.0835753448553316, 1.0696918340539345, 1.061596111019881]\n",
      "Labels:  tensor([5, 5, 8, 9, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0010, Accuracy: 8761/10000 (87.61%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Conditional Entropy: 100%|██████████| 4915/4915 [00:00<00:00, 6400.68it/s]\n",
      "Entropy: 100%|██████████| 4915/4915 [00:00<00:00, 6557.22it/s]\n",
      "Training Set Size:  90%|█████████ | 90/100 [22:47<04:34, 27.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset indices:  [59416 47328   424 39319 49957]\n",
      "Scores:  [1.1478207978123125, 1.1102614257709287, 1.0850558392752996, 1.0790079513168136, 1.0625752859965933]\n",
      "Labels:  tensor([3, 8, 9, 9, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0008, Accuracy: 8889/10000 (88.89%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Conditional Entropy: 100%|██████████| 4910/4910 [00:00<00:00, 6348.30it/s]\n",
      "Entropy: 100%|██████████| 4910/4910 [00:00<00:00, 6736.72it/s]\n",
      "Training Set Size:  95%|█████████▌| 95/100 [25:29<02:24, 28.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset indices:  [24416 55128 23434 25783 18240]\n",
      "Scores:  [1.027956026336538, 1.0255406982760356, 1.021273391633068, 1.00988963365211, 1.0074447158415576]\n",
      "Labels:  tensor([8, 8, 5, 0, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0008, Accuracy: 8840/10000 (88.40%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Conditional Entropy: 100%|██████████| 4905/4905 [00:00<00:00, 6711.24it/s]\n",
      "Entropy: 100%|██████████| 4905/4905 [00:00<00:00, 6212.90it/s]\n",
      "Training Set Size: 100%|██████████| 100/100 [28:13<00:00, 30.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset indices:  [42221   813 16036 32880  3644]\n",
      "Scores:  [1.116252837355666, 1.0832360593485681, 1.0686570135491935, 1.0640626741446475, 1.05782307283321]\n",
      "Labels:  tensor([5, 2, 8, 0, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0007, Accuracy: 9013/10000 (90.13%)\n"
     ]
    }
   ],
   "source": [
    "# Run experiment\n",
    "test_accs = []\n",
    "test_loss = []\n",
    "added_indices = []\n",
    "\n",
    "pbar = tqdm(initial=len(active_learning_data.training_dataset), total=max_training_samples, desc=\"Training Set Size\")\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "while True:\n",
    "    model = BayesianConvNet(num_classes).to(device=device)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Train\n",
    "    for data, target in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        data = data.to(device=device)\n",
    "        target = target.to(device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        prediction = model(data, 1).squeeze(1)\n",
    "        loss = loss_fn(prediction, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Test\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, desc=\"Testing\", leave=False):\n",
    "            data = data.to(device=device)\n",
    "            target = target.to(device=device)\n",
    "\n",
    "            prediction = torch.logsumexp(model(data, num_test_inference_samples), dim=1) - math.log(\n",
    "                num_test_inference_samples\n",
    "            )\n",
    "            loss += loss_fn(prediction, target)\n",
    "\n",
    "            prediction = prediction.max(1)[1]\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "\n",
    "    loss /= len(test_loader.dataset)\n",
    "    test_loss.append(loss)\n",
    "\n",
    "    percentage_correct = 100.0 * correct / len(test_loader.dataset)\n",
    "    test_accs.append(percentage_correct)\n",
    "\n",
    "    print(\n",
    "        \"Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\".format(\n",
    "            loss, correct, len(test_loader.dataset), percentage_correct\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if len(active_learning_data.training_dataset) >= max_training_samples:\n",
    "        break\n",
    "\n",
    "    # Acquire pool predictions\n",
    "    N = len(active_learning_data.pool_dataset)\n",
    "    logits_N_K_C = torch.empty((N, num_inference_samples, num_classes), dtype=torch.double, pin_memory=use_cuda)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for i, (data, _) in enumerate(tqdm(pool_loader, desc=\"Evaluating Acquisition Set\", leave=False)):\n",
    "            data = data.to(device=device)\n",
    "\n",
    "            lower = i * pool_loader.batch_size\n",
    "            upper = min(lower + pool_loader.batch_size, N)\n",
    "            logits_N_K_C[lower:upper].copy_(model(data, num_inference_samples).double(), non_blocking=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        candidate_batch = batchbald.get_bald_batch(\n",
    "            logits_N_K_C, acquisition_batch_size, dtype=torch.double, device=device\n",
    "        )\n",
    "\n",
    "    targets = repeated_mnist.get_targets(active_learning_data.pool_dataset)\n",
    "    dataset_indices = active_learning_data.get_dataset_indices(candidate_batch.indices)\n",
    "\n",
    "    print(\"Dataset indices: \", dataset_indices)\n",
    "    print(\"Scores: \", candidate_batch.scores)\n",
    "    print(\"Labels: \", targets[candidate_batch.indices])\n",
    "\n",
    "    active_learning_data.acquire(candidate_batch.indices)\n",
    "    added_indices.append(dataset_indices)\n",
    "    pbar.update(len(dataset_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(start\u001b[38;5;241m=\u001b[39mnum_initial_samples, stop\u001b[38;5;241m=\u001b[39mmax_training_samples \u001b[38;5;241m+\u001b[39m acquisition_batch_size, step\u001b[38;5;241m=\u001b[39macquisition_batch_size), test_accs)\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Set Size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(np.arange(start=num_initial_samples, stop=max_training_samples + acquisition_batch_size, step=acquisition_batch_size), test_accs)\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.hlines(90, num_initial_samples, max_training_samples, colors='r', linestyles='dashed')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = {\n",
    "    'num_initial_samples': num_initial_samples,\n",
    "    'num_classes': num_classes,\n",
    "    'max_training_samples': max_training_samples,\n",
    "    'acquisition_batch_size': acquisition_batch_size,\n",
    "    'num_inference_samples': num_inference_samples,\n",
    "    'num_test_inference_samples': num_test_inference_samples,\n",
    "    'num_samples': num_samples,\n",
    "    'test_batch_size': test_batch_size,\n",
    "    'batch_size': batch_size,\n",
    "    'scoring_batch_size': scoring_batch_size,\n",
    "    'training_iterations': training_iterations\n",
    "}\n",
    "\n",
    "save_experiment('Lenet5-simple_BALD', params_dict, {\n",
    "    'test_accs': test_accs,\n",
    "    'test_loss': test_loss,\n",
    "    'added_indices': added_indices\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
