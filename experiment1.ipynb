{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from batchbald_redux import repeated_mnist, active_learning, batchbald\n",
    "from main.models import BayesianConvNet\n",
    "from main.utils import save_experiment, load_experiment\n",
    "\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:64: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.\n",
      "  warnings.warn(\"`data_source` argument is not used and will be removed in 2.2.0.\"\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "train_dataset, test_dataset = repeated_mnist.create_MNIST_dataset()\n",
    "\n",
    "# number of initial samples \n",
    "num_initial_samples = 40\n",
    "num_classes = 10\n",
    "\n",
    "# get indices of initial samples\n",
    "initial_samples = active_learning.get_balanced_sample_indices(\n",
    "    repeated_mnist.get_targets(train_dataset), num_classes=num_classes, n_per_digit=num_initial_samples / num_classes\n",
    ")\n",
    "\n",
    "# Experiment parameters\n",
    "max_training_samples = 100  # Maximum number of samples to acquire from the pool dataset \n",
    "acquisition_batch_size = 5  # Number of samples to acquire in each acquisition step\n",
    "num_inference_samples = 50  # Number of samples to use for inference in MC-Dropout\n",
    "num_test_inference_samples = 5  \n",
    "num_samples = 100000 # Number of samples to use for estimation in batchbald\n",
    "\n",
    "test_batch_size = 512  # Batch size for testing\n",
    "batch_size = 64  # Batch size for training\n",
    "scoring_batch_size = 128  # Batch size for scoring \n",
    "training_iterations = 4096 * 6 # Number of training iterations (batches) to run\n",
    "\n",
    "kwargs = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = \"cuda\" if use_cuda else \"cpu\"\n",
    "\n",
    "print(f\"use_cuda: {use_cuda}\")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "active_learning_data = active_learning.ActiveLearningData(train_dataset)\n",
    "\n",
    "# Split off the initial samples first.\n",
    "active_learning_data.acquire(initial_samples)\n",
    "\n",
    "# THIS REMOVES MOST OF THE POOL DATA. UNCOMMENT THIS TO TAKE ALL UNLABELLED DATA INTO ACCOUNT!\n",
    "active_learning_data.extract_dataset_from_pool(55000)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    active_learning_data.training_dataset,\n",
    "    sampler=active_learning.RandomFixedLengthSampler(active_learning_data.training_dataset, training_iterations),\n",
    "    batch_size=batch_size,\n",
    "    **kwargs,\n",
    ")\n",
    "\n",
    "pool_loader = torch.utils.data.DataLoader(\n",
    "    active_learning_data.pool_dataset, batch_size=scoring_batch_size, shuffle=False, **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Set Size:  40%|████      | 40/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0028, Accuracy: 7141/10000 (71.41%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Conditional Entropy: 100%|██████████| 4960/4960 [00:00<00:00, 7657.08it/s]\n",
      "Entropy: 100%|██████████| 4960/4960 [00:00<00:00, 9624.61it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([1.9551, 1.9063, 1.9008, 1.8993, 1.8902, 1.8838, 1.8633, 1.8504, 1.8457,\n",
      "        1.8390], dtype=torch.float64),\n",
      "indices=tensor([3019, 1456, 2373, 3520,  517, 2681,  286, 3108,  907,  430]))\n",
      "torch.return_types.topk(\n",
      "values=tensor([1.2654, 1.1127, 1.0929, 1.0828, 1.0283, 0.9894, 0.9873, 0.9672, 0.9671,\n",
      "        0.9641], dtype=torch.float64),\n",
      "indices=tensor([3019, 1696, 3520, 1086, 2976, 3206, 2681,  517, 2357,  430]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entropy: 100%|██████████| 4960/4960 [00:00<00:00, 9483.36it/s] \n",
      "Training Set Size:  45%|████▌     | 45/100 [01:04<11:47, 12.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset indices:  [23472 28944 42327 41217 46580]\n",
      "Scores:  [1.25087769893981, 1.19062686371772, 1.1860350331984852, 1.1561641162276248, 1.1340433182090535]\n",
      "Labels:  tensor([2, 2, 2, 2, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0025, Accuracy: 7268/10000 (72.68%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Conditional Entropy: 100%|██████████| 4955/4955 [00:00<00:00, 8842.26it/s] \n",
      "Entropy: 100%|██████████| 4955/4955 [00:00<00:00, 9157.48it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([1.9814, 1.9566, 1.9539, 1.8923, 1.8859, 1.8807, 1.8695, 1.8576, 1.8460,\n",
      "        1.8431], dtype=torch.float64),\n",
      "indices=tensor([1411, 4318,  598, 1696, 3503, 3930, 2974, 4785, 4638,  967]))\n",
      "torch.return_types.topk(\n",
      "values=tensor([1.3288, 1.2746, 1.2679, 1.1955, 1.1505, 1.1452, 1.1250, 1.1045, 1.0791,\n",
      "        1.0724], dtype=torch.float64),\n",
      "indices=tensor([1696, 2974, 4318, 1086, 3017,  289, 1311, 4508, 1790,  598]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entropy: 100%|██████████| 4955/4955 [00:00<00:00, 8931.24it/s] \n",
      "Training Set Size:  50%|█████     | 50/100 [02:05<10:25, 12.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset indices:  [50075 52669 54740 14104  3895]\n",
      "Scores:  [1.2328700166982527, 1.169212225878496, 1.1631054219063395, 1.1572460527253698, 1.151517323101395]\n",
      "Labels:  tensor([7, 7, 7, 7, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0027, Accuracy: 7158/10000 (71.58%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Conditional Entropy: 100%|██████████| 4950/4950 [00:00<00:00, 8557.80it/s] \n",
      "Entropy: 100%|██████████| 4950/4950 [00:00<00:00, 9089.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([1.9419, 1.9084, 1.9045, 1.8867, 1.8861, 1.8844, 1.8672, 1.8422, 1.8408,\n",
      "        1.8295], dtype=torch.float64),\n",
      "indices=tensor([3515,   75,  966, 1975, 1918, 4633, 1694, 1745, 2133, 3308]))\n",
      "torch.return_types.topk(\n",
      "values=tensor([1.3825, 1.2546, 1.1980, 1.1734, 1.1660, 1.1361, 1.1272, 1.1015, 1.0772,\n",
      "        1.0626], dtype=torch.float64),\n",
      "indices=tensor([1694, 4504,  289, 4633, 1309, 4315, 3015, 1085, 1788, 1006]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entropy: 100%|██████████| 4950/4950 [00:00<00:00, 9317.37it/s] \n",
      "Training Set Size:  55%|█████▌    | 55/100 [03:09<09:30, 12.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset indices:  [ 9994 16756 13096 53017 39835]\n",
      "Scores:  [1.2516065547946824, 1.2395376705061483, 1.2328000787566271, 1.1818893445182925, 1.1347621975508049]\n",
      "Labels:  tensor([0, 7, 9, 2, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0023, Accuracy: 7352/10000 (73.52%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 71\u001b[0m\n\u001b[0;32m     69\u001b[0m         lower \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m*\u001b[39m pool_loader\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m     70\u001b[0m         upper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(lower \u001b[38;5;241m+\u001b[39m pool_loader\u001b[38;5;241m.\u001b[39mbatch_size, N)\n\u001b[1;32m---> 71\u001b[0m         logits_N_K_C[lower:upper]\u001b[38;5;241m.\u001b[39mcopy_(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_inference_samples\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdouble(), non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     74\u001b[0m     candidate_batch \u001b[38;5;241m=\u001b[39m batchbald\u001b[38;5;241m.\u001b[39mget_bald_batch(\n\u001b[0;32m     75\u001b[0m         logits_N_K_C, acquisition_batch_size, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdouble, device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[0;32m     76\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\vince\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vince\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vince\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\batchbald_redux\\consistent_mc_dropout.py:29\u001b[0m, in \u001b[0;36mBayesianModule.forward\u001b[1;34m(self, input_B, k)\u001b[0m\n\u001b[0;32m     26\u001b[0m BayesianModule\u001b[38;5;241m.\u001b[39mk \u001b[38;5;241m=\u001b[39m k\n\u001b[0;32m     28\u001b[0m mc_input_BK \u001b[38;5;241m=\u001b[39m BayesianModule\u001b[38;5;241m.\u001b[39mmc_tensor(input_B, k)\n\u001b[1;32m---> 29\u001b[0m mc_output_BK \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmc_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmc_input_BK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m mc_output_B_K \u001b[38;5;241m=\u001b[39m BayesianModule\u001b[38;5;241m.\u001b[39munflatten_tensor(mc_output_BK, k)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mc_output_B_K\n",
      "File \u001b[1;32mc:\\Users\\vince\\Documents\\Statistics\\TT\\msc_thesis\\main\\models.py:22\u001b[0m, in \u001b[0;36mBayesianConvNet.mc_forward_impl\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmc_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1_drop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(F\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2_drop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(\u001b[38;5;28minput\u001b[39m)), \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1024\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\vince\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_jit_internal.py:499\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vince\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:796\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    795\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run experiment\n",
    "test_accs = []\n",
    "test_loss = []\n",
    "added_indices = []\n",
    "\n",
    "pbar = tqdm(initial=len(active_learning_data.training_dataset), total=max_training_samples, desc=\"Training Set Size\")\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "while True:\n",
    "    model = BayesianConvNet(num_classes).to(device=device)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Train\n",
    "    for data, target in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        data = data.to(device=device)\n",
    "        target = target.to(device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        prediction = model(data, 1).squeeze(1)\n",
    "        loss = loss_fn(prediction, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Test\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, desc=\"Testing\", leave=False):\n",
    "            data = data.to(device=device)\n",
    "            target = target.to(device=device)\n",
    "\n",
    "            prediction = torch.logsumexp(model(data, num_test_inference_samples), dim=1) - math.log(\n",
    "                num_test_inference_samples\n",
    "            )\n",
    "            loss += loss_fn(prediction, target)\n",
    "\n",
    "            prediction = prediction.max(1)[1]\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "\n",
    "    loss /= len(test_loader.dataset)\n",
    "    test_loss.append(loss)\n",
    "\n",
    "    percentage_correct = 100.0 * correct / len(test_loader.dataset)\n",
    "    test_accs.append(percentage_correct)\n",
    "\n",
    "    print(\n",
    "        \"Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\".format(\n",
    "            loss, correct, len(test_loader.dataset), percentage_correct\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if len(active_learning_data.training_dataset) >= max_training_samples:\n",
    "        break\n",
    "\n",
    "    # Acquire pool predictions\n",
    "    N = len(active_learning_data.pool_dataset)\n",
    "    logits_N_K_C = torch.empty((N, num_inference_samples, num_classes), dtype=torch.double, pin_memory=use_cuda)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for i, (data, _) in enumerate(tqdm(pool_loader, desc=\"Evaluating Acquisition Set\", leave=False)):\n",
    "            data = data.to(device=device)\n",
    "\n",
    "            lower = i * pool_loader.batch_size\n",
    "            upper = min(lower + pool_loader.batch_size, N)\n",
    "            logits_N_K_C[lower:upper].copy_(model(data, num_inference_samples).double(), non_blocking=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        candidate_batch = batchbald.get_bald_batch(\n",
    "            logits_N_K_C, acquisition_batch_size, dtype=torch.double, device=device\n",
    "        )\n",
    "\n",
    "    targets = repeated_mnist.get_targets(active_learning_data.pool_dataset)\n",
    "    dataset_indices = active_learning_data.get_dataset_indices(candidate_batch.indices)\n",
    "\n",
    "    print(\"Dataset indices: \", dataset_indices)\n",
    "    print(\"Scores: \", candidate_batch.scores)\n",
    "    print(\"Labels: \", targets[candidate_batch.indices])\n",
    "\n",
    "    active_learning_data.acquire(candidate_batch.indices)\n",
    "    added_indices.append(dataset_indices)\n",
    "    pbar.update(len(dataset_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(start\u001b[38;5;241m=\u001b[39mnum_initial_samples, stop\u001b[38;5;241m=\u001b[39mmax_training_samples \u001b[38;5;241m+\u001b[39m acquisition_batch_size, step\u001b[38;5;241m=\u001b[39macquisition_batch_size), test_accs)\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Set Size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(np.arange(start=num_initial_samples, stop=max_training_samples + acquisition_batch_size, step=acquisition_batch_size), test_accs)\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.hlines(90, num_initial_samples, max_training_samples, colors='r', linestyles='dashed')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = {\n",
    "    'num_initial_samples': num_initial_samples,\n",
    "    'num_classes': num_classes,\n",
    "    'max_training_samples': max_training_samples,\n",
    "    'acquisition_batch_size': acquisition_batch_size,\n",
    "    'num_inference_samples': num_inference_samples,\n",
    "    'num_test_inference_samples': num_test_inference_samples,\n",
    "    'num_samples': num_samples,\n",
    "    'test_batch_size': test_batch_size,\n",
    "    'batch_size': batch_size,\n",
    "    'scoring_batch_size': scoring_batch_size,\n",
    "    'training_iterations': training_iterations\n",
    "}\n",
    "\n",
    "save_experiment('Lenet5-simple_BALD', params_dict, {\n",
    "    'test_accs': test_accs,\n",
    "    'test_loss': test_loss,\n",
    "    'added_indices': added_indices\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
